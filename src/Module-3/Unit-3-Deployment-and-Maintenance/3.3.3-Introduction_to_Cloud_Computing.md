# An Introduction to Cloud Computing

You will hear people talking about 'the cloud'. The cloud is a major component of modern computing, yet the term is very vague and ephemeral, literally like the clouds. As a software engineer you need to know something of what the cloud is. Most of the software and services you work on will run there and the way you write code and build apps needs to take that final environment into consideration.

We are going to look at a series of topics that we think will give you a basic understanding of what the cloud is and how to think about and design the software that will run on in. We are going to look at:

* Virtualization
* Clustering
* Networking
* Scaling

## Hypervisors for Virtualization

The first thing to understand is virtualization. The idea is that on your computer your can create and use another computer that exists only in software. A virtual machine.

### Virtual Machines

The place to start getting familiar with this idea is to install and use VirtualBox. We are going to provision and create a virtual raspberry pi in VirtualBox.

* Download and install [VirtualBox](https://www.virtualbox.org/wiki/Downloads) for your host system i.e. Windows
* Download the [Raspberry Pi Desktop](https://www.raspberrypi.com/software/raspberry-pi-desktop/).iso image
* In VirtualBox create a new VM (Virtual Machine) and name it, select type __Linux__, version __Debian(64-bit)__
* Choose 'create' and accept all the default options
* In the settings of the new VM goto __storage__ in the section for the optical disc select the .iso image for the raspberry pi you just downloaded
* When you start the machine it will boot from this - select install from the list
* Finally you'll see you now can run a little computer inside your big computer!

This VM will behave like a distinct computer, for example it has it's own ip address on your network, you can install software on it.

### Bare Metal Hypervisors

If we take the trick of VirtualBox and apply it in a more purest way we might wonder why have an operating system at all? A 'Bare Metal' hypervisor is installed in the place of an operating system i.e. Windows or Linux. The hypervisor is a thin layer of software that is installed on disc and exposes the motherboard's components to the abstraction of virtualization. All you get when you turn the computer on and plug in a monitor is an ip address. You have to use another computer to connect to the hypervisor, and then you can start to make virtual machines on the 'bare metal'.

> That means you pay for 1 server, but you can run 4 virtual machines on that 1 server. 

![4 VMs](https://user-images.githubusercontent.com/4499581/138301496-481c708c-2314-48e9-b86b-2acf3d65b1b1.jpg)

Before virtualization you usually had one server dedicated to one service i.e. one server would run the company email. You would install something like Windows Server or a Linux distro and then install all the software for the email server. With virtualization you can do much more with your hardware, and run multiple operating systems on one piece of hardware.

This moment of separating the operating system from the hardware is a fundamental building block of cloud computing. Why do you think that might be? 

## Clustering

When you think about a virtualized server what you actually have is a cluster of virtual computers. They all share the limited underlying resources like RAM and disc space. So they are bound together from that point of view. However they don't know about each other. Each VM is a full operating system and it runs no differently from your OS (it doesn't know its only a VM).

If we want to group a number of computers together and get them to work as a team we need to start clustering them. In clusters computers are usually referred to as nodes. You can expect to find 2 different types of nodes:

* Manager nodes
* Worker nodes

The manager nodes do the work of managing the cluster, labeling and keeping track of worker nodes and services. There might be a number of manager nodes so one can be swapped out if need be and everything will keep running. Same with worker nodes. In a cluster you want to be able to power down any node without impacting the services, and be able to then provision and join a replacement node into the cluster if you want to.

If you can replace nodes why can't you add them too? You can. This is called horizontal scaling. You start with a cluster of 3 nodes and as demand for your services increases you can match that demand by adding nodes. For example in the autumn/fall Walmarts has a spike in website traffic around '[black friday](https://en.wikipedia.org/wiki/Black_Friday_(shopping))' so they anticipate the extra demand by scaling horizontally adding extra nodes. After the holiday season Walmart don't need all those extra nodes as demand reduces, so to save the costs of keeping all that availability there they can scale back and remove the extra nodes. You often here infrastructure set up like this as 'elastic'.

## Networking

* load balancers
* Ingres
* declarative
* self-healing

## Assignment

You will need:

* VirtualBox or 3-4 raspberry pi
* Raspberry pi image burned onto 4 SDcards
* 4 nodes with docker installed on each node

### Setup

* create images on the SD cards
* enable ssh (touch ssh in /boot on SD card)
* Assemble pies, power, networking if building physically
* find ip addresses of your raspberry pies n your network (Lanscan)
* update hostnames on each pi (sudo nano /etc/hosts & sudo nano /etc/hostname)
* `sudo apt-get update && sudo apt-get upgrade`
* install docker on each node `curl -sSL https://get.docker.com | sh`
* install docker on VirtualBox pi `curl -fsSL https://get.docker.com -o get-docker.sh` || `sudo apt-get install docker.io`
* (you may need this step) `sudo apt-get update --allow-releaseinfo-change`
* modify the user by adding pi to the docker group `sudo usermod -aG docker pi`
* check all is well with `docker version`
* VirtualBox right-click and clone - generate new MAC addresses - full clone
* VirtualBox settings -> general -> advanced -> shared clipboard -> bidirectional
* https://labs.play-with-docker.com/

### Cluster your pies

* Pick the manager node
* `sudo docker swarm init --advertise-addr NODE1_IP_ADDRESS`
* Save the token somewhere safe
```
docker swarm join --token SWMTKN-1-49nj1cmql0jkz5s954yi3o...3ojnwacrr2e7c NODE1_IP_ADDRESS:2377
```
* Join the worker nodes to the swarm
* Add portainer to the manager node to monitor the swarm `curl -L https://downloads.portainer.io/portainer-agent-stack.yml -o portainer-agent-stack.yml`
* deploy the portainer monitor  `docker stack deploy -c portainer-agent-stack.yml portainer`
* check your cluster status `https://NODE1_IP_ADDRESS:9443`

### Compute in the cluster

* network overlay `docker network create --driver overlay main_network`
* `docker network ls`
* `docker service logs service_name`

```yaml
version: "3.7"

services:
  cms:
    image: drupal
    networks:
      - main_network
    ports:
      - 80:80
  db:
    image: postgres:13
    ports:
      - 5432:5432
    volumes:
      - db:/var/lib/postgresql/data
    environment:
      - POSTGRES_USER=pi
      - POSTGRES_PASSWORD=password
      - POSTGRES_DATABASE=drupal
    deploy:
      placement:
        constraints:
          - node.role == manager
    networks:
      - main_network

volumes:
  db:

networks:
  main_network:
    external: true
```
Database setup use database address `db`.
* any cluster ip address will resolve to drupal's port number
* which node is it actually running on
* Ingress Routing Mesh
* Replicas
* 12 factor app [read](https://madewithlove.com/blog/software-engineering/12-factors-in-the-era-of-containers/) [simpler-read](https://medium.com/notbinary/the-twelve-factor-container-8d1edc2a49d4)

```yaml
version: '3'

services:
    app:
        image: ghost:latest
        ports: 
            - "2368:2368"
        restart: always
        deploy:
          replicas: 3
        networks:
          - ghost_network
networks:
    ghost_network:
        external:
```

