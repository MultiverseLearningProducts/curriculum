# An Introduction to Cloud Computing

You will hear people talking about 'the cloud'. The cloud is a major component of modern computing, yet the term is very vague and ephemeral, literally like the clouds. As a software engineer you need to know something of what the cloud is. Most of the software and services you work on will run there and the way you write code and build apps needs to take that final environment into consideration.

We are going to look at a series of topics that we think will give you a basic understanding of what the cloud is and how to think about and design the software that will run on in. We are going to look at:

* Virtualization
* Clustering
* Networking
* Scaling

## Hypervisors for Virtualization

The first thing to understand is virtualization. The idea is that on your computer your can create and use another computer that exists only in software. A virtual machine.

### Virtual Machines

The place to start getting familiar with this idea is to install and use VirtualBox. We are going to provision and create a virtual raspberry pi in VirtualBox.

* Download and install [VirtualBox](https://www.virtualbox.org/wiki/Downloads) for your host system i.e. Windows
* Download the [Raspberry Pi Desktop](https://www.raspberrypi.com/software/raspberry-pi-desktop/).iso image
* In VirtualBox create a new VM (Virtual Machine) and name it, select type __Linux__, version __Debian(64-bit)__
* Choose 'create' and accept all the default options
* In the settings of the new VM goto __storage__ in the section for the optical disc select the .iso image for the raspberry pi you just downloaded
* When you start the machine it will boot from this - select __install__ from the list
* When it comes to select the GRUB loader choose `/dev/sda (ata-VBOX_HARDDISK...)`
* Finally you'll see you now can run a little computer inside your big computer!

This VM will behave like a distinct computer, for example it has it's own ip address on your network, you can install software on it.

### Bare Metal Hypervisors

If we take the trick of VirtualBox and apply it in a more purest way we might wonder why have an operating system at all? A 'Bare Metal' hypervisor is installed in the place of an operating system i.e. Windows or Linux. The hypervisor is a thin layer of software that is installed on disc and exposes the motherboard's components to the abstraction of virtualization. All you get when you turn the computer on and plug in a monitor is an ip address. You have to use another computer to connect to the hypervisor, and then you can start to make virtual machines on the 'bare metal'.

> That means you pay for 1 server, but you can run 4 virtual machines on that 1 server. 

![4 VMs](https://user-images.githubusercontent.com/4499581/138301496-481c708c-2314-48e9-b86b-2acf3d65b1b1.jpg)

Before virtualization you usually had one server dedicated to one service i.e. one server would run the company email. You would install something like Windows Server or a Linux distro and then install all the software for the email server. With virtualization you can do much more with your hardware, and run multiple operating systems on one piece of hardware.

This moment of separating the operating system from the hardware is a fundamental building block of cloud computing. Why do you think that might be? 

## Clustering

When you think about a virtualized server what you actually have is a cluster of virtual computers. They all share the limited underlying resources like RAM and disc space. So they are bound together from that point of view. However they don't know about each other. Each VM is a full operating system and it runs no differently from your OS (it doesn't know its only a VM).

If we want to group a number of computers together and get them to work as a team we need to start clustering them. In clusters computers are usually referred to as nodes. You can expect to find 2 different types of nodes:

* Manager nodes
* Worker nodes

The manager nodes do the work of managing the cluster, labeling and keeping track of worker nodes and services. There might be a number of manager nodes so one can be swapped out if need be and everything will keep running. Same with worker nodes. In a cluster you want to be able to power down any node without impacting the services, and be able to then provision and join a replacement node into the cluster if you want to.

If you can replace nodes why can't you add them too? You can. This is called horizontal scaling. You start with a cluster of 3 nodes and as demand for your services increases you can match that demand by adding nodes. For example in the autumn/fall Walmarts has a spike in website traffic around '[black friday](https://en.wikipedia.org/wiki/Black_Friday_(shopping))' so they anticipate the extra demand by scaling horizontally adding extra nodes. After the holiday season Walmart don't need all those extra nodes as demand reduces, so to save the costs of keeping all that availability there they can scale back and remove the extra nodes. You often here infrastructure set up like this described as 'elastic'.

### Set up your own cluster

There are different ways you can create a cluster for yourself. You can pay for VMs on [AWS](https://aws.amazon.com/), [DigitalOcean](https://www.digitalocean.com/), [Azure](https://azure.microsoft.com/en-gb/free/) or [Scaleway](https://www.scaleway.com/en/) (Scaleway is ARM based servers). Whatever service you use you'll need to
do the following:

* create at least 3 nodes
* enable ssh (usually you have to add your public ssh key __BEFORE__ you create your node)
* install a flavour of Linux like Ubuntu, Debian or Busybox (Alpine)
* find the ip addresses of each node and make a note of them `ifconfig`
* name each node by updating `/etc/hosts` and `/etc/hostname` i.e. `192.168.56.101 manager01`

#### VirtualBox

* Install ruby
* Install [Vagrant](https://www.vagrantup.com/downloads)
* create a `Vagrantfile` and add the following
```ruby
# -*- mode: ruby -*-
# vi: set ft=ruby :

$install_docker_script = <<SCRIPT
echo "Installing dependencies ..."
sudo apt-get update
echo Installing Docker...
curl -sSL https://get.docker.com/ | sh
sudo usermod -aG docker vagrant
SCRIPT

BOX_NAME = "ubuntu/xenial64"
MEMORY = "512"
MANAGERS = 2
MANAGER_IP = "172.20.20.1"
WORKERS = 2
WORKER_IP = "172.20.20.10"
CPUS = 2
VAGRANTFILE_API_VERSION = "2"

Vagrant.configure(VAGRANTFILE_API_VERSION) do |config|
    #Common setup
    config.vm.box = BOX_NAME
    config.vm.synced_folder ".", "/vagrant"
    config.vm.provision "shell",inline: $install_docker_script, privileged: true
    config.vm.provider "virtualbox" do |vb|
      vb.memory = MEMORY
      vb.cpus = CPUS
    end
    #Setup Manager Nodes
    (1..MANAGERS).each do |i|
        config.vm.define "manager0#{i}" do |manager|
          manager.vm.network :private_network, ip: "#{MANAGER_IP}#{i}"
          manager.vm.hostname = "manager0#{i}"
          if i == 1
            #Only configure port to host for Manager01
            config.vm.network :forwarded_port, guest: 80, host: 80
            config.vm.network :forwarded_port, guest: 5432, host: 5432
            config.vm.network :forwarded_port, guest: 2368, host: 2368
            manager.vm.network :forwarded_port, guest: 8080, host: 8080
            manager.vm.network :forwarded_port, guest: 5000, host: 5000
            manager.vm.network :forwarded_port, guest: 9000, host: 9000
          end
        end
    end
    #Setup Woker Nodes
    (1..WORKERS).each do |i|
        config.vm.define "worker0#{i}" do |worker|
            worker.vm.network :private_network, ip: "#{WORKER_IP}#{i}"
            worker.vm.hostname = "worker0#{i}"
        end
    end
end
```
* `vagrant up`
* `vagrant ssh manager01`
* `vagrant ssh manager02`
* `vagrant ssh worker01`
* `vagrant ssh worker02`

#### Play With Docker

* [https://labs.play-with-docker.com/](https://labs.play-with-docker.com/)
* Login with your Docker account (create one if you need to)
* Start a session (you have 4 hours of compute time)
* Add instances (they will already have ip addresses and names)

#### Create the Cluster

Once you have your set of nodes ready select a 'manager' node and run the following docker command to initialise that node as the manager. 
```sh
`sudo docker swarm init --advertise-addr NODE1_IP_ADDRESS`
```
Swap `NODE1_IP_ADDRESS` with the ip address of your manager node. You'll get a command to then run on your worker nodes (see below) so copy and paste this so your other worker nodes join the 'swarm' (Docker's name for a cluster).

```sh
docker swarm join --token SWMTKN-1-0b8xy6wvz5vs1ep0j1bkedrhfkncqumeqc57s3bvftbx053ecx-5vkjneteyuy26luxxj9fdh9vz 192.168.56.111:2377
```

#### Visualize the Cluster

You've just created a cluster! To begin to appreciate what you have just done we are going to download and install a tool to help us monitor and manager our cluster. There is an open source project called [Portainer](https://www.portainer.io/) that can help us with this. On the __manager node__ run the following command to download the 'stack' file.
```sh
curl -L https://downloads.portainer.io/portainer-agent-stack.yml -o portainer-agent-stack.yml
```
If you `cat portainer-agent-stack.yml` you will recognise this is a docker compose file. However there are some extensions that make this a 'stack' file. Can you see the section labeled `deploy:` because we are going to run this docker compose file in a swarm (cluster) we have some extra config, like how many replicas of this service we will run, can the service be deployed on any node or must it be a particular type of node, etc.

When you are ready deploy this stack into your cluster.
```sh
docker stack deploy -c portainer-agent-stack.yml portainer
```
If you are using [https://labs.play-with-docker.com/](https://labs.play-with-docker.com/) you'll see some ports will open at the top of your screen, click on the __9443__ port - you might need to add `https://` to the beginning of the URL to get it to load, its doing some fancy things with ssh tunnels and SSL is not going to work so ignore the warnings - your good - you know you are doing something a little funky.

If you have a cluster of VMs visit `https://NODE1_IP_ADDRESS:9000` where `NODE1_IP_ADDRESS` is the ip address of your manager node.

* Make the admin account
* Navigate to Swarm visualise the cluster

## Networking

* load balancers
* Ingres
* declarative
* self-healing


### Setup

* create images on the SD cards
* enable ssh (touch ssh in /boot on SD card)
* Assemble pies, power, networking if building physically
* find ip addresses of your raspberry pies n your network (Lanscan)
* update hostnames on each pi (sudo nano /etc/hosts & sudo nano /etc/hostname)
* `sudo apt-get update && sudo apt-get upgrade`
* install docker on each node `sudo curl -sSL https://get.docker.com | sh`
* install docker on VirtualBox pi `curl -fsSL https://get.docker.com -o get-docker.sh` || `sudo apt-get install docker.io`
* (you may need this step) `sudo apt-get update --allow-releaseinfo-change`
* modify the user by adding pi to the docker group `sudo usermod -aG docker pi`
* check all is well with `docker version`
* VirtualBox right-click and clone - generate new MAC addresses - full clone
* VirtualBox settings -> general -> advanced -> shared clipboard -> bidirectional
* https://labs.play-with-docker.com/

### Cluster your pies

* Pick the manager node
* `sudo docker swarm init --advertise-addr NODE1_IP_ADDRESS`
* Save the token somewhere safe
```
docker swarm join --token SWMTKN-1-49nj1cmql0jkz5s954yi3o...3ojnwacrr2e7c NODE1_IP_ADDRESS:2377
```
* Join the worker nodes to the swarm
* Add portainer to the manager node to monitor the swarm `curl -L https://downloads.portainer.io/portainer-agent-stack.yml -o portainer-agent-stack.yml`
* deploy the portainer monitor  `docker stack deploy -c portainer-agent-stack.yml portainer`
* check your cluster status `https://NODE1_IP_ADDRESS:9443`

### Compute in the cluster

* network overlay `docker network create --driver overlay ghost_network`
* `docker network ls`
* `docker service logs service_name`

```yaml
version: '3.7'

services:
  blog:
    image: ghost:4-alpine
    depends_on:
      - db
    ports: 
        - "2368:2368"
    environment:
      database__client: mysql
      database__connection__host: db
      database__connection__user: ghostusr
      database__connection__password: ghostme
      database__connection__database: ghostdata
    deploy:
      replicas: 3
    networks:
      - ghost_network
    volumes:
      - "/mnt/content:/var/lib/ghost/content"
  db:
    image: mysql:5.7
    ports:
      - "3306:3306"
    environment:
      MYSQL_ROOT_PASSWORD: example
      MYSQL_USER: ghostusr
      MYSQL_PASSWORD: ghostme
      MYSQL_DATABASE: ghostdata
    deploy:
      replicas: 1
      placement:
        constraints:
          - "node.role==manager"
    networks:
      - ghost_network
    volumes:
      - "/mnt/mysql:/var/lib/mysql"
networks:
    ghost_network:
        external: true
```


### Vagrant

* `vagrant up`
* `vagrant status`
```sh
172.20.20.11 manager01
172.20.20.12 manager02
172.20.20.101 worker01
172.20.20.102 worker02
```
* `vagrant ssh manager01` init swarm `docker swarm init --listen-addr 172.20.20.11:2377 --advertise-addr 172.20.20.11:2377`
* `vagrant ssh worker01` join workers
* `docker swarm join-token manager`
* `curl -L https://downloads.portainer.io/portainer-agent-stack.yml -o portainer-agent-stack.yml`
* `docker stack deploy --compose-file=portainer-agent-stack.yml portainer`
* [localhost:9000](http://localhost:9000)
* wait for containers to be pulled `docker service ls`

### GlusterFS

#### Install Setup
* [instructions](https://thenewstack.io/tutorial-create-a-docker-swarm-with-persistent-storage-using-glusterfs/)
* [each node] `sudo add-apt-repository ppa:gluster/glusterfs-3.10 && sudo apt-get update`
* [each node] `sudo apt-get install glusterfs-server -y`
* [each node] `sudo systemctl start glusterfs-server && sudo systemctl enable glusterfs-server`

#### Create the disk spaces (optional)
* `sudo fdisk /dev/sdb` n p 1 w
* `sudo mkfs.ext4 /dev/sdb1`
* `sudo mkdir /glusterfs`
* `sudo mount /dev/sdb1 /glusterfs`
* add `/dev/sdb1 /glusterfs ext4 defaults 0 0` to `/etc/fstab`

#### Create the volumes
* `sudo gluster peer probe [manager01, manager02, worker01, worker02]`
* `mkdir ~/vols/content` `mkdir ~/vols/mysql`
* `sudo gluster volume create vols replica 4 manager01:/home/vagrant/vols manager02:/home/vagrant/vols worker01:/home/vagrant/vols worker02:/home/vagrant/vols force`
* `sudo gluster volume start vols` `sudo gluster volume start vols`
* `sudo mount.glusterfs localhost:/vols /mnt`
* add `localhost:/vols /mnt glusterfs defaults,_netdev,backupvolfile-server=localhost 0 0` to `/etc/fstab`

#### Expose to Docker
* `docker plugin install --alias glusterfs trajano/glusterfs-volume-plugin --grant-all-permissions --disable`
* `docker plugin set glusterfs SERVERS=172.20.20.11,172.20.20.12,172.20.20.101,172.20.20.102`
* `docker plugin enable glusterfs`

Create a folder named /brick on gluster1.example.com
Create a folder named /brick on gluster2.example.com
Now create your gluster volume named testvol using gluster1.example.com:/brick and gluster2.example.com/brick
you will need to start the volume too.
next you need to mount your new testvol in your filesystem on gluster1.example.com and gluster2.example.com. Execute something like:

```sql
CREATE USER 'ghostusr'@'%' IDENTIFIED BY 'ghostme';
GRANT ALL PRIVILEGES ON ghostdata.* TO 'ghostusr'@'%';
```

## The problems of clustering

Now we have created our own cloud environment. We can deploy and scale our applications across multiple nodes! Sadly it is not that straight forward. We can't just replicate multiple instances of a service across nodes and have it just work. Any app that writes or reads from disc might not scale well. For example, wordpress writes your configuration into the file system, and also a database. You'll see the glitchy behavior in the ghost blog example, you can't create an account. This is because the underlying file system is on separate computers. I might write some config to the file system on one computer, but my next request for another page in the cluster bounces me to a different computer - and I've lost my state.

### Abstracting the file system into 'volumes'

For our cluster to work and for us to be able to scale apps we need to also abstract the file system we are using and have it replicated on each node. For this we'll use [GlusterFS](https://www.gluster.org/). What we are going to do on each node is create the same folder i.e. `/home/vagrant/vols`. Next we are going to install GlusterFS and create a 'volume' which will represent this folder in an abstraction. We can treat the 'volume' as if it were a single folder on a single computer. That way we can just include it in our docker stack file as a mounted volume.

## 12 Factor apps

The point of introducing you to cloud computing like this is so you get to see some of the powers and problems of cloud computing. Developers targeting deployment in clustered environments like this have developed a set of guidelines to make the elastic scaling of apps easier to manage. We think you should know about these [12 factors](https://12factor.net/) as they might influence your design choices and the way you structure your features and solutions.

1. __Codebase__ One codebase tracked in revision control, many deploys
1. __Dependencies__ Explicitly declare and isolate dependencies
1. __Config__ Store config in the environment
1. __Backing services__ Treat backing services as attached resources
1. __Build, release, run__ Strictly separate build and run stages
1. __Processes__ Execute the app as one or more stateless processes
1. __Port binding__ Export services via port binding
1. __Concurrency__ Scale out via the process model
1. __Disposability__ Maximize robustness with fast startup and graceful shutdown
1. __Dev/prod parity__ Keep development, staging, and production as similar as possible
1. __Logs__ Treat logs as event streams
1. __Admin processes__ Run admin/management tasks as one-off processes

## Assignment

What do these 12 factors mean to you? We'd like you to contrast the 12 factors with what you do in your workplace. 